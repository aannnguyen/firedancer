/* fd_vm_jitproto is a first draft of a sBPF JIT compiler for
   Firedancer.  Nothing to see here, it's broken and work-in-progress.

   This version of the JIT compiler supports a linear memory mapping
   only. */

#define _GNU_SOURCE
#include "../../fd_flamenco_base.h"

/* Include dynasm headers.  These fail to compile when some strict
   checks are enabled. */
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wconversion"
#pragma GCC diagnostic ignored "-Wsign-conversion"
#pragma GCC diagnostic ignored "-Wimplicit-fallthrough"
#include "dasm_proto.h"
#include "dasm_x86.h"
#pragma GCC diagnostic pop
| .arch x64

#include "../../../ballet/sbpf/fd_sbpf_instr.h"
#include "../../../ballet/sbpf/fd_sbpf_loader.h"
#include "../../runtime/fd_acc_mgr.h"
#include "../../runtime/context/fd_exec_epoch_ctx.h"
#include "../../runtime/context/fd_exec_slot_ctx.h"
#include "../../runtime/context/fd_exec_txn_ctx.h"
#include "../../runtime/sysvar/fd_sysvar_recent_hashes.h"
#include "../fd_vm_private.h"

#include <assert.h>
#include <setjmp.h>
#include <errno.h>
#include <stdio.h>
#include <sys/mman.h>
#include <sys/stat.h>

/* FD_DASM_R{...} specify the dynasm register index of x86_64 registers. */

#define FD_DASM_RAX  (0)
#define FD_DASM_RCX  (1)
#define FD_DASM_RDX  (2)
#define FD_DASM_RBX  (3)
#define FD_DASM_RSP  (4)
#define FD_DASM_RBP  (5)
#define FD_DASM_RSI  (6)
#define FD_DASM_RDI  (7)
#define FD_DASM_R8   (8)
#define FD_DASM_R9   (9)
#define FD_DASM_R10 (10)
#define FD_DASM_R11 (11)
#define FD_DASM_R12 (12)
#define FD_DASM_R13 (13)
#define FD_DASM_R14 (14)
#define FD_DASM_R15 (15)

/* FD_VM_JIT_SEGMENT_MAX is the max number of segments. */

#define FD_VM_JIT_SEGMENT_MAX (64)

/* Thread-local storage ***************************************************

   For now, these are assumed to be absolute-addressed using the fs segment
   selector.  Practically, this means that fd_vm_jitproto only supports
   targets with FD_HAS_THREADS.  (Other targets might use absolute
   addressing without a segment selector or rip-relative) */

static FD_TL fd_vm_t * fd_jit_vm  = NULL;  /* current VM being executed */

/* Thread-local storage for address translation

   fd_jit_segment_cnt is number of memory regions mapped in by the VM.
   fd_jit_mem_{ro,rw}_sz are the number of read- and write-addressable
   bytes in each region.  fd_jit_mem_base points to the first byte of a
   region in host address space. */

static FD_TL uint  fd_jit_segment_cnt;
static FD_TL uint  fd_jit_mem_ro_sz[ FD_VM_JIT_SEGMENT_MAX ];
static FD_TL uint  fd_jit_mem_rw_sz[ FD_VM_JIT_SEGMENT_MAX ];
static FD_TL ulong fd_jit_mem_base [ FD_VM_JIT_SEGMENT_MAX ];

/* Thread-local storage for fast return to JIT entrypoint
   These are a setjmp()-like anchor for quickly exiting out of a VM
   execution, e.g. in case of a VM fault.
   Slots: 0=rbx 1=rbp 2=r12 3=r13 4=r14 5=r15 6=rsp 7=rip */

static FD_TL ulong fd_jit_jmp_buf[8];

/* Thread-local storage for exception handling */

static FD_TL ulong fd_jit_segfault_vaddr;
static FD_TL ulong fd_jit_segfault_rip;

//static FD_TL ulong     ic_correct = 0UL;   /* number of lddw instructions executed */


/* Mapping between sBPF registers and x86_64 registers ********************

   This mapping is valid just before a translated sBPF instruction is
   about to be executed.  (At the `=>next_label` token in the code gen
   loop)

   BPF | r0  | r1  | r2  | r3  | r4  | r5  | r6  | r7  | r8  | r9  | r10
   X86 | rsi | r11 | r12 | r13 | r14 | r15 | rbx | rcx | r8  | r9  | r10

   x86_64 GPRs rax, rdi, rdx, rbp do not map to sBPF registers.  Those can
   be used as scratch registers for complex opcodes.

   Note that this mapping cannot be trivially changed.  Certain x86
   instructions (like div) have hardcoded register accesses which the
   JIT code works around.

   dynasm macros bpf_r{...} resolve to 64-bit register names.

   reg_bpf2x86 is indexed by sBPF register numbers and resolves to the
   x86_64 dynasm register index. */

static uchar const reg_bpf2x86[11] = {
  [ 0] =     FD_DASM_RSI,
  | .define bpf_r0,  rsi
  [ 1] =     FD_DASM_R11,
  | .define bpf_r1,  r11
  [ 2] =     FD_DASM_R12,
  | .define bpf_r2,  r12
  [ 3] =     FD_DASM_R13,
  | .define bpf_r3,  r13
  [ 4] =     FD_DASM_R14,
  | .define bpf_r4,  r14
  [ 5] =     FD_DASM_R15,
  | .define bpf_r5,  r15
  [ 6] =     FD_DASM_RBX,
  | .define bpf_r6,  rbx
  [ 7] =     FD_DASM_RCX,
  | .define bpf_r7,  rcx
  [ 8] =     FD_DASM_R8,
  | .define bpf_r8,  r8
  [ 9] =     FD_DASM_R9,
  | .define bpf_r9,  r9
  [10] =     FD_DASM_R10
  | .define bpf_r10, r10
};

/* GDB JIT debug interface ***********************************************/

#define GDB_JIT_NOACTION      0
#define GDB_JIT_REGISTER_FN   1
#define GDB_JIT_UNREGISTER_FN 2

struct gdb_jit_code_entry {
  struct gdb_jit_code_entry * next_entry;
  struct gdb_jit_code_entry * prev_entry;
  char const *                symfile_addr;
  ulong                       symfile_size;
};

struct gdb_jit_descriptor {
  uint version;
  uint action_flag;
  struct gdb_jit_code_entry * relevant_entry;
  struct gdb_jit_code_entry * first_entry;
};

/* GDB puts a breakpoint in this function. */
void __attribute__((noinline)) __jit_debug_register_code(void) {}

struct gdb_jit_descriptor __jit_debug_descriptor = { 1, 0, 0, 0 };


/* New proposed ABI */

struct account_meta {
  /* 0x00 */ uint8_t pubkey[0x20];
  /* 0x20 */ uint8_t owner[0x20];
  /* 0x40 */ uint64_t data;
  /* 0x48 */ uint64_t data_len;
  /* 0x50 */ uint64_t lamports;
  /* 0x58 */ uint64_t flags;
  /* 0x60 */ uint8_t unused[0x20];
};

typedef struct account_meta account_meta_t;

/* fd_jit_labels is a table of function pointers to 'static' labels in the
   JIT code.  They are indexed by fd_jit_lbl_{...}. */

| .globals fd_jit_lbl_
static FD_TL void * fd_jit_labels[ fd_jit_lbl__MAX ];

/* fd_jit_entrypoint is the entrypoint function of JIT compiled code.
   first_rip is a pointer to the x86 instruction in the host address space
   that corresponds to the BPF entrypoint. */

typedef int (* fd_jit_entrypoint_t)( ulong first_rip );

int
main( int     argc,
      char ** argv ) {
  fd_boot( &argc, &argv );
  char const * bin_path = fd_env_strip_cmdline_cstr( &argc, &argv, "--program-file", NULL, NULL );

  /* Read and parse ELF binary */

  FILE * bin_file = fopen( bin_path, "r" );
  if( FD_UNLIKELY( !bin_file ) )
    FD_LOG_ERR(( "fopen(\"%s\") failed (%i-%s)", bin_path, errno, fd_io_strerror( errno ) ));

  struct stat bin_stat;
  if( FD_UNLIKELY( 0!=fstat( fileno( bin_file ), &bin_stat ) ) )
    FD_LOG_ERR(( "fstat() failed (%i-%s)", errno, fd_io_strerror( errno ) ));
  if( FD_UNLIKELY( !S_ISREG( bin_stat.st_mode ) ) )
    FD_LOG_ERR(( "File \"%s\" not a regular file", bin_path ));

  ulong  bin_sz  = (ulong)bin_stat.st_size;
  void * bin_buf = malloc( bin_sz+8UL );
  if( FD_UNLIKELY( !bin_buf ) )
    FD_LOG_ERR(( "malloc(%#lx) failed (%i-%s)", bin_sz, errno, fd_io_strerror( errno ) ));

  if( FD_UNLIKELY( fread( bin_buf, bin_sz, 1UL, bin_file )!=1UL ) )
    FD_LOG_ERR(( "fread() failed (%i-%s)", errno, fd_io_strerror( errno ) ));
  FD_TEST( 0==fclose( bin_file ) );

  int is_deploy = 0;
  fd_sbpf_elf_info_t elf_info;
  FD_TEST( fd_sbpf_elf_peek( &elf_info, bin_buf, bin_sz, is_deploy ) );

  void * rodata = malloc( elf_info.rodata_footprint );
  FD_TEST( rodata );

  ulong  prog_align     = fd_sbpf_program_align();
  ulong  prog_footprint = fd_sbpf_program_footprint( &elf_info );
  fd_sbpf_program_t * prog = fd_sbpf_program_new( aligned_alloc( prog_align, prog_footprint ), &elf_info, rodata );
  FD_TEST( prog );

  fd_sbpf_syscalls_t * syscalls = fd_sbpf_syscalls_new(
      aligned_alloc( fd_sbpf_syscalls_align(), fd_sbpf_syscalls_footprint() ) );
  FD_TEST( syscalls );

  fd_vm_syscall_register_all( syscalls, is_deploy );

  if( FD_UNLIKELY( 0!=fd_sbpf_program_load( prog, bin_buf, bin_sz, syscalls, is_deploy ) ) )
    FD_LOG_ERR(( "fd_sbpf_program_load() failed: %s", fd_sbpf_strerror() ));

  /* Create workspace and scratch allocator */

  ulong cpu_idx = fd_tile_cpu_id( fd_tile_idx() );
  if( cpu_idx>=fd_shmem_cpu_cnt() ) cpu_idx = 0UL;
  fd_wksp_t * wksp = fd_wksp_new_anonymous( FD_SHMEM_NORMAL_PAGE_SZ, 65536, fd_shmem_cpu_idx( fd_shmem_numa_idx( cpu_idx ) ), "wksp", 0UL );
  assert( wksp );

  ulong   smax = 1UL<<30;  /* 1 GiB */
  uchar * smem = malloc( smax );
  ulong fmem[ 64 ];
  fd_scratch_attach( smem, fmem, smax, 64UL );
  fd_scratch_push();

  /* Create runtime context */

  ulong txn_max = 2UL;
  ulong rec_max = 1024UL;
  fd_funk_t * funk = fd_funk_join( fd_funk_new( fd_wksp_alloc_laddr( wksp, fd_funk_align(), fd_funk_footprint(), 1UL ), 1UL, (ulong)fd_tickcount(), txn_max, rec_max ) );
  assert( funk );
  fd_funk_start_write( funk );

  fd_acc_mgr_t * acc_mgr = fd_acc_mgr_new( fd_scratch_alloc( FD_ACC_MGR_ALIGN, FD_ACC_MGR_FOOTPRINT ), funk );
  assert( acc_mgr );

  fd_funk_txn_xid_t xid[1] = {{ .ul = {0,1} }};

  fd_funk_txn_t * funk_txn = fd_funk_txn_prepare( funk, NULL, xid, 1 );
  assert( funk_txn );
  fd_scratch_push();

  ulong vote_acct_max = 1;
  uchar * epoch_ctx_mem = fd_scratch_alloc( fd_exec_epoch_ctx_align(), fd_exec_epoch_ctx_footprint( vote_acct_max ) );
  uchar * slot_ctx_mem  = fd_scratch_alloc( FD_EXEC_SLOT_CTX_ALIGN,  FD_EXEC_SLOT_CTX_FOOTPRINT  );
  uchar * txn_ctx_mem   = fd_scratch_alloc( FD_EXEC_TXN_CTX_ALIGN,   FD_EXEC_TXN_CTX_FOOTPRINT   );

  fd_exec_epoch_ctx_t * epoch_ctx = fd_exec_epoch_ctx_join( fd_exec_epoch_ctx_new( epoch_ctx_mem, vote_acct_max ) );
  fd_exec_slot_ctx_t *  slot_ctx  = fd_exec_slot_ctx_join ( fd_exec_slot_ctx_new ( slot_ctx_mem, fd_scratch_virtual() ) );
  fd_exec_txn_ctx_t *   txn_ctx   = fd_exec_txn_ctx_join  ( fd_exec_txn_ctx_new  ( txn_ctx_mem ) );

  assert( epoch_ctx );
  assert( slot_ctx  );

  epoch_ctx->epoch_bank.rent.lamports_per_uint8_year = 3480;
  epoch_ctx->epoch_bank.rent.exemption_threshold = 2;
  epoch_ctx->epoch_bank.rent.burn_percent = 50;

  fd_features_enable_all( &epoch_ctx->features );

  slot_ctx->epoch_ctx = epoch_ctx;
  slot_ctx->funk_txn  = funk_txn;
  slot_ctx->acc_mgr   = acc_mgr;
  slot_ctx->valloc    = fd_scratch_virtual();

  fd_slot_bank_new( &slot_ctx->slot_bank );
  fd_block_block_hash_entry_t * recent_block_hashes = deq_fd_block_block_hash_entry_t_alloc( slot_ctx->valloc, FD_SYSVAR_RECENT_HASHES_CAP );
  slot_ctx->slot_bank.recent_block_hashes.hashes = recent_block_hashes;
  fd_block_block_hash_entry_t * recent_block_hash = deq_fd_block_block_hash_entry_t_push_tail_nocopy( recent_block_hashes );
  fd_memset( recent_block_hash, 0, sizeof(fd_block_block_hash_entry_t) );

  txn_ctx->epoch_ctx = epoch_ctx;
  txn_ctx->slot_ctx  = slot_ctx;
  txn_ctx->funk_txn  = funk_txn;
  txn_ctx->acc_mgr   = acc_mgr;
  txn_ctx->valloc    = fd_scratch_virtual();

  ulong cu_avail = ULONG_MAX;
  txn_ctx->compute_meter      = cu_avail;
  txn_ctx->compute_unit_limit = cu_avail;

  fd_exec_instr_ctx_t instr_ctx[1] = {{0}};
  instr_ctx->epoch_ctx = epoch_ctx;
  instr_ctx->slot_ctx  = slot_ctx;
  instr_ctx->txn_ctx   = txn_ctx;

  /* Set up VM */

  fd_sha256_t _sha[1];
  fd_sha256_t * sha = fd_sha256_join( fd_sha256_new( _sha ) );

  /* Set up accounts */

# define ACC1_SZ 0x100000
  uchar * account1 = fd_scratch_alloc( 32, ACC1_SZ );
  memset( account1, 0, ACC1_SZ );
  uchar account2[ 32 ] = {0};
  account_meta_t metas[2] = {
    {
      .pubkey   = {0},
      .owner    = {0},
      .data     = 3UL<<32,
      .data_len = ACC1_SZ,
      .lamports = 1000000000,
      .flags    = 0
    },
    {
      .pubkey   = {0},
      .owner    = {0},
      .data     = 4UL<<32,
      .data_len = sizeof(account2),
      .lamports = 1000000000,
      .flags    = 0
    }
  };

  /* Region 0: Sentinel */
  fd_jit_mem_ro_sz[0] = 0;
  fd_jit_mem_rw_sz[0] = 0;
  fd_jit_mem_base [0] = 0;

  /* Region 1: Stack */
# define VM_STACK_SZ 0x10000UL
  void * stack_mem = fd_scratch_alloc( 0x10, VM_STACK_SZ );
  fd_jit_mem_ro_sz[1] = VM_STACK_SZ;
  fd_jit_mem_rw_sz[1] = VM_STACK_SZ;
  fd_jit_mem_base [1] = (ulong)stack_mem;

  /* Region 2: Metadata table */
  fd_jit_mem_ro_sz[2] = sizeof(metas);
  fd_jit_mem_rw_sz[2] = sizeof(metas);
  fd_jit_mem_base [2] = (ulong)metas;

  /* Region 3: Account */
  fd_jit_mem_ro_sz[3] = (uint)metas[0].data_len;
  fd_jit_mem_rw_sz[3] = 0;
  fd_jit_mem_base [3] = (ulong)account1;

  /* Region 4: Account */
  fd_jit_mem_ro_sz[4] = sizeof(account2);
  fd_jit_mem_rw_sz[4] = sizeof(account2);
  fd_jit_mem_base [4] = (ulong)account2;

  fd_jit_segment_cnt = 5U;

  fd_vm_input_region_t mem_regions[5];
  for( uint j=0U; j<fd_jit_segment_cnt; j++ ) {
    mem_regions[j] = (fd_vm_input_region_t) {
      .vaddr_offset = ((ulong)j)<<32,
      .haddr        = fd_jit_mem_base [j],
      .region_sz    = fd_jit_mem_ro_sz[j],
      .is_writable  = fd_jit_mem_rw_sz[j] > 0
    };
  }

  fd_vm_t * vm = fd_vm_join( fd_vm_new( fd_scratch_alloc( fd_vm_align(), fd_vm_footprint() ) ) );
  FD_TEST( vm );
  // ulong event_max = 1UL<<20;
  // ulong event_data_max = 2048UL;
  //fd_vm_trace_t * trace = fd_vm_trace_new( aligned_alloc( fd_vm_trace_align(), fd_vm_trace_footprint( event_max, event_data_max ) ), event_max, event_data_max );
  FD_TEST( fd_vm_init(
    vm,
    instr_ctx,
    FD_VM_HEAP_DEFAULT,  /* heap_max */
    txn_ctx->compute_meter, /* entry_cu */
    rodata, /* rodata */
    elf_info.rodata_sz, /* rodata_sz */
    prog->text, /* text */
    prog->text_cnt, /* text_cnt */
    prog->text_off, /* text_off */
    prog->text_sz,  /* text_sz */
    prog->entry_pc, /* entry_pc */
    prog->calldests, /* calldests */
    syscalls,
    NULL,// trace,
    sha,
    mem_regions,
    fd_jit_segment_cnt,
    NULL, /* acc_region_metas */
    0 /* is_deprecated */ ) );

  vm->reg[ 1] = 2UL<<32; /* account table address */
  vm->reg[ 2] = 2; /* account count */
  vm->reg[ 3] = 3UL<<32; /* instruction data address */
  vm->reg[ 4] = 0; /* instruction data size */
  vm->reg[10] = (1UL<<32) + 0x1000;

  for( uint j=0U; j<fd_jit_segment_cnt; j++ ) {
    vm->region_haddr[j] = fd_jit_mem_base[j];
    vm->region_ld_sz[j] = fd_jit_mem_ro_sz[j];
    vm->region_st_sz[j] = fd_jit_mem_rw_sz[j];
  }

  fd_jit_vm = vm;

  /* Set up dynasm */

  dasm_State * d;

  | .section code
  dasm_init( &d, DASM_MAXSECTION );

  dasm_setupglobal( &d, fd_jit_labels, fd_jit_lbl__MAX );

  dasm_growpc( &d, (uint)prog->text_cnt );
  int next_label = 0;

  | .actionlist actions
  dasm_setup( &d, actions );

  dasm_State ** Dst = &d;

  /* Start emitting code */

  | .code

  /* Exception handlers */

  |->vm_fault:
  | mov edi, 999
  | jmp ->longjmp

  /* Derive offsets of thread locals in FS "segment" */

# if defined(__FSGSBASE__)
  ulong fs_base; __asm__( "mov %%fs:0, %0" : "=r"(fs_base) );
# else
  ulong fs_base = __builtin_ia32_rdfsbase64();
# endif
# define FS_RELATIVE(ptr) ((uint)( (ulong)(ptr) - fs_base ))
  uint  fd_jit_vm_tpoff             = FS_RELATIVE( &fd_jit_vm             );
  uint  fd_jit_segment_cnt_tpoff    = FS_RELATIVE( &fd_jit_segment_cnt    );
  uint  fd_jit_mem_ro_sz_tpoff      = FS_RELATIVE( fd_jit_mem_ro_sz       );
  uint  fd_jit_mem_rw_sz_tpoff      = FS_RELATIVE( fd_jit_mem_rw_sz       );
  uint  fd_jit_mem_base_tpoff       = FS_RELATIVE( fd_jit_mem_base        );
  uint  fd_jit_segfault_vaddr_tpoff = FS_RELATIVE( &fd_jit_segfault_vaddr );
  uint  fd_jit_segfault_rip_tpoff   = FS_RELATIVE( &fd_jit_segfault_rip   );
# undef FD_RELATIVE

  /* Address translation macros

     The translate_{rw,ro}_{1,2,4,8} macros perform address translation
     and access permission checks for {read-write,read-only} accesses of
     {1,2,4,8} bytes.  The compiler may inline this macro for each
     translated sBPF instruction, so these should be optimized for small
     size.

     Prior to the macro, rdi holds an address in the virtual address
     space (untrusted in [0,2^64)).  If translation and permission
     checks succeed, rdx holds the translated address in the host
     address space.  On failure jumps to sigsegv.  Reasons for failure
     include access to out-of-bounds memory, unaligned address, access
     permission error. */

  | .define translate_in,  rdi
  | .define translate_out, rdx
  |.macro gen_scalar_translate, sz_table_tpoff
  | // rdi := virtual address
  | // ebp := size of the access minus 1
  |
  | // edx := segment offset
  | mov edx, edi
  |
  | // edi := segment index
  | shr rdi, 32
  |
  | // segment index in bounds?
  | fs
  | cmp edi, [fd_jit_segment_cnt_tpoff]
  | jae ->translate_fail
  |
  | // aligned access?
  | mov eax, edx
  | and eax, ebp
  | test eax, eax
  | jnz ->translate_fail
  |
  | // no multi segment overlap?
  | add ebp, edx
  | jc ->translate_fail
  |
  | // segment offset in bounds?
  | fs
  | cmp ebp, [rdi*4 + sz_table_tpoff]
  | jae ->translate_fail
  |
  | // rdx := host address
  | fs
  | add rdx, [rdi*8 + fd_jit_mem_base_tpoff]
  | ret
  |.endmacro

  |->translate_fail:
  | shl rdi, 32
  | or rdi, rdx
  | fs
  | mov [fd_jit_segfault_vaddr_tpoff], rdi
  | mov rdi, [rsp]
  | fs
  | mov [fd_jit_segfault_rip_tpoff], rdi
  | jmp ->vm_fault

  |->fd_jit_vm_translate_rw:
  | gen_scalar_translate, fd_jit_mem_rw_sz_tpoff
  |->fd_jit_vm_translate_ro:
  | gen_scalar_translate, fd_jit_mem_ro_sz_tpoff

  |.macro translate_rw_1
  | xor ebp, ebp
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_rw_2
  | mov ebp, 1
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_rw_4
  | mov ebp, 3
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_rw_8
  | mov ebp, 7
  | call ->fd_jit_vm_translate_rw
  |.endmacro

  |.macro translate_ro_1
  | xor ebp, ebp
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |.macro translate_ro_2
  | mov ebp, 1
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |.macro translate_ro_4
  | mov ebp, 3
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |.macro translate_ro_8
  | mov ebp, 7
  | call ->fd_jit_vm_translate_ro
  |.endmacro

  |->save_regs:
  | mov64 rax, (ulong)vm
  | mov [rax + offsetof(fd_vm_t, reg[ 0])], bpf_r0
  | mov [rax + offsetof(fd_vm_t, reg[ 1])], bpf_r1
  | mov [rax + offsetof(fd_vm_t, reg[ 2])], bpf_r2
  | mov [rax + offsetof(fd_vm_t, reg[ 3])], bpf_r3
  | mov [rax + offsetof(fd_vm_t, reg[ 4])], bpf_r4
  | mov [rax + offsetof(fd_vm_t, reg[ 5])], bpf_r5
  | mov [rax + offsetof(fd_vm_t, reg[ 6])], bpf_r6
  | mov [rax + offsetof(fd_vm_t, reg[ 7])], bpf_r7
  | mov [rax + offsetof(fd_vm_t, reg[ 8])], bpf_r8
  | mov [rax + offsetof(fd_vm_t, reg[ 9])], bpf_r9
  | mov [rax + offsetof(fd_vm_t, reg[10])], bpf_r10
  | ret

  |->restore_regs:
  | mov64 rax, (ulong)vm
  | mov bpf_r0,  [rax + offsetof(fd_vm_t, reg[ 0])]
  | mov bpf_r1,  [rax + offsetof(fd_vm_t, reg[ 1])]
  | mov bpf_r2,  [rax + offsetof(fd_vm_t, reg[ 2])]
  | mov bpf_r3,  [rax + offsetof(fd_vm_t, reg[ 3])]
  | mov bpf_r4,  [rax + offsetof(fd_vm_t, reg[ 4])]
  | mov bpf_r5,  [rax + offsetof(fd_vm_t, reg[ 5])]
  | mov bpf_r6,  [rax + offsetof(fd_vm_t, reg[ 6])]
  | mov bpf_r7,  [rax + offsetof(fd_vm_t, reg[ 7])]
  | mov bpf_r8,  [rax + offsetof(fd_vm_t, reg[ 8])]
  | mov bpf_r9,  [rax + offsetof(fd_vm_t, reg[ 9])]
  | mov bpf_r10, [rax + offsetof(fd_vm_t, reg[10])]
  | ret

  /* Generate setjmp/longjmp subroutines.  These can be called from any
     execution state with a valid stack.  The JIT uses them to restore a
     sane SystemV-ABI context when exiting JIT code.

     These are based on musl libc's setjmp/longjmp implementation.
     Copyright 2011-2012 Nicholas J. Kain, licensed under standard MIT license

     setjmp takes no arguments.  longjmp takes a 64-bit value in rdi.
     When setjmp returns from setjmp, sets rax=0 and rdx=0.  When setjmp
     returns from longjmp, sets rax to the rdi argument of longjmp, and
     sets rdx=1.  setjmp preserves rdi. */

  |->setjmp:
  | mov64 r11, (ulong)fd_jit_jmp_buf
  | mov [r11+ 0], rbx
  | mov [r11+ 8], rbp
  | mov [r11+16], r12
  | mov [r11+24], r13
  | mov [r11+32], r14
  | mov [r11+40], r15
  | // save callee's stack pointer
  | // derived by removing our 8 byte stack frame (only return address)
  | lea rdx, [rsp+8]
  | mov [r11+48], rdx
  | // save return address
  | mov rdx, [rsp]
  | mov [r11+56], rdx
  | // normal return
  | xor eax, eax
  | xor edx, edx
  | ret

  |->longjmp:
  | mov rax, rdi // move first argument to first output register
  | mov edx, 1   // set second output register to 1
  | mov64 rdi, (ulong)fd_jit_jmp_buf
  | // restore execution state to callee of setjmp
  | mov rbx, [rdi+ 0]
  | mov rbp, [rdi+ 8]
  | mov r12, [rdi+16]
  | mov r13, [rdi+24]
  | mov r14, [rdi+32]
  | mov r15, [rdi+40]
  | mov rsp, [rdi+48]
  | push qword [rdi+56]
  | ret // retpoline

  /* The emulate_syscall function switches from a JIT to an interpreter (C)
     execution context and invokes a syscall handler.  Register edi is
     assumed to hold the byte offset into the vm->syscalls table of the
     fd_sbpf_syscalls_t entry to invoke.
     On syscall return, switches back to the JIT execution context and
     resumes execution after the syscall instruction. */

  |->emulate_syscall:
  | call ->save_regs
  | // rax points to the BPF register file
  | mov rbp, rsp
  | // Reserve 16 aligned bytes on the stack
  | and rsp, -16
  | sub rsp, 16
  | fs
  | mov r11, [fd_jit_vm_tpoff]
  | mov r10, [r11 + offsetof(fd_vm_t, syscalls)]
  | mov r10, [r10 + rdi + offsetof(fd_sbpf_syscalls_t, func)]
  | mov rdi, r11
  | // load BPF r1 through r5 into function arguments
  | // FIXME could avoid spill to memory by shuffling registers
  | mov rsi, [rax + offsetof(fd_vm_t, reg[1])]
  | mov rdx, [rax + offsetof(fd_vm_t, reg[2])]
  | mov rcx, [rax + offsetof(fd_vm_t, reg[3])]
  | mov r8,  [rax + offsetof(fd_vm_t, reg[4])]
  | mov r9,  [rax + offsetof(fd_vm_t, reg[5])]
  | lea r11, [rax + offsetof(fd_vm_t, reg[0])]
  | push r11
  | call r10
  | mov rsp, rbp
  | call ->restore_regs
  | test edi, edi
  | jnz ->vm_fault
  | ret

  /* The call_stack_push function pushes the current program counter and
     eBPF registers r6, r7, r8, r9 to the shadow stack.  The frame register
     (r10) grows upwards.  FIXME implement shadow stack overflow. */

# define REG(n) (offsetof(fd_vm_t, shadow[0].r##n))

  |->call_stack_push:
  | fs
  | mov rdi, [fd_jit_vm_tpoff]
  | mov esi, [rdi + offsetof(fd_vm_t, frame_cnt)]
  | // vm->frame_cnt++
  | lea eax, [esi+1]
  | mov [rdi + offsetof(fd_vm_t, frame_cnt)], eax
  | // save registers
  | shl esi, FD_VM_STACK_FRAME_LG_MAX
  | mov [rdi+rsi+REG(6)], bpf_r6
  | mov [rdi+rsi+REG(7)], bpf_r7
  | mov [rdi+rsi+REG(8)], bpf_r8
  | mov [rdi+rsi+REG(9)], bpf_r9
  | ret

  /* The call_stack_pop function undoes the effects of call_stack_push. */

  |->call_stack_pop:
  | fs
  | mov rdi, [fd_jit_vm_tpoff]
  | mov esi, [rdi + offsetof(fd_vm_t, frame_cnt)]
  | // vm->frame_cnt--
  | dec esi
  | mov [rdi + offsetof(fd_vm_t, frame_cnt)], esi
  | // restore registers
  | shl esi, FD_VM_STACK_FRAME_LG_MAX
  | mov bpf_r6, [rdi+esi+REG(6)]
  | mov bpf_r7, [rdi+esi+REG(7)]
  | mov bpf_r8, [rdi+esi+REG(8)]
  | mov bpf_r9, [rdi+esi+REG(9)]
  | ret

# undef REG

  /* Start translating user code */

  |->entrypoint:

  /* Create setjmp anchor used to return from JIT */

  | call ->setjmp // preserves rdi
  | test edx, edx
  | jnz ->return_to_callee

  /* Enter JIT execution context */

  | call ->restore_regs
  | call rdi
  | mov rdi, bpf_r0
  | call ->longjmp
  |->return_to_callee:
  | ret

  //ulong         text_skip  = prog->text_off >> 3;
  ulong * const text_start = prog->text;
  ulong *       text_end   = prog->text + prog->text_cnt;

  int bpf_label_off = next_label;

  for( ulong * cur=text_start; cur<text_end; cur++ ) {
    ulong instr = *cur;

    ulong opcode  = fd_vm_instr_opcode( instr ); /* in [0,256) even if malformed */
    ulong dst     = fd_vm_instr_dst   ( instr ); /* in [0, 16) even if malformed */
    ulong src     = fd_vm_instr_src   ( instr ); /* in [0, 16) even if malformed */
    short offset  = fd_vm_instr_offset( instr ); /* in [-2^15,2^15) even if malformed */
    uint  imm     = fd_vm_instr_imm   ( instr ); /* in [0,2^32) even if malformed */

    /* Macros for translating register accesses */

    uint x86_dst = reg_bpf2x86[ dst ];
    uint x86_src = reg_bpf2x86[ src ];

    | .define dst64, Rq(x86_dst)
    | .define src64, Rq(x86_src)
    | .define dst32, Rd(x86_dst)
    | .define src32, Rd(x86_src)
    | .define src8,  Rb(x86_src)

    /* Macro for translating jumps */

    ulong * jmp_dst       = cur + 1 + offset; /* may be OOB, FIXME validate */
    int     jmp_dst_lbl   = bpf_label_off + (int)( jmp_dst - text_start );
    //int     jmp_bounds_ok = jmp_dst>=text_start && jmp<text_end;
    /* FIXME do a bounds check */
    /* FIXME what happens if the label is not set? */

    /* FIXME CU accounting */

    /* Create a dynamic label for each instruction */

    uint cur_pc = (uint)( cur - text_start );
    next_label = bpf_label_off + (int)cur_pc;
    |=>next_label:

    /* Translate instruction */

    switch( opcode ) {

    /* 0x00 - 0x0f ******************************************************/

    case 0x04:  /* FD_SBPF_OP_ADD_IMM */
      | add dst32, imm
      break;

    case 0x05:  /* FD_SBPF_OP_JA */
      | jmp =>jmp_dst_lbl
      break;

    case 0x07:  /* FD_SBPF_OP_ADD64_IMM */
      | add dst64, imm
      break;

    case 0x0c:  /* FD_SBPF_OP_ADD_REG */
      | add dst32, src32
      break;

    case 0x0f:  /* FD_SBPF_OP_ADD64_REG */
      | add dst64, src64
      break;

    /* 0x10 - 0x1f ******************************************************/

    case 0x14:  /* FD_SBPF_OP_SUB_IMM */
      | sub dst32, imm
      break;

    case 0x15:  /* FD_SBPF_OP_JEQ_IMM */
      | cmp dst64, imm
      /* pre branch check here ... branchless cu update? */
      | je =>jmp_dst_lbl
      break;

    case 0x17:  /* FD_SBPF_OP_SUB64_IMM */
      | sub dst64, imm
      break;

    case 0x18:  /* FD_SBPF_OP_LDQ */
      cur++; {
      ulong imm64 = (ulong)imm | ( (ulong)fd_vm_instr_imm( *cur ) << 32 );
      if( imm64==0 ) {
        | xor dst32, dst32
      } else {
        | mov dst64, imm64
      }
      break;
    }

    case 0x1c:  /* FD_SBPF_OP_SUB_REG */
      | sub dst32, src32
      break;

    case 0x1d:  /* FD_SBPF_OP_JEQ_REG */
      | cmp dst64, src64
      | je =>jmp_dst_lbl
      break;

    case 0x1f:  /* FD_SBPF_OP_SUB64_REG */
      | sub dst64, src64
      break;

    /* 0x20 - 0x2f ******************************************************/

    case 0x24:  /* FD_SBPF_OP_MUL_IMM */
      /* TODO strength reduction? */
      | imul dst32, imm
      break;

    case 0x25:  /* FD_SBPF_OP_JGT_IMM */
      | cmp dst64, imm
      | ja =>jmp_dst_lbl
      break;

    case 0x27:  /* FD_SBPF_OP_MUL64_IMM */
      /* TODO strength reduction? */
      | imul dst64, imm
      break;

    case 0x2c:  /* FD_SBPF_OP_MUL_REG */
      | imul dst32, src32
      break;

    case 0x2d:  /* FD_SBPF_OP_JGT_REG */
      | cmp dst64, src64
      | ja =>jmp_dst_lbl
      break;

    case 0x2f:  /* FD_SBPF_OP_MUL64_REG */
      | imul dst64, src64
      break;

    /* 0x30 - 0x3f ******************************************************/

    case 0x34:  /* FD_SBPF_OP_DIV_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | mov edi, imm
      | div edi
      | xchg eax, dst32
      break;

    case 0x35:  /* FD_SBPF_OP_JGE_IMM */
      | cmp dst64, imm
      | jae =>jmp_dst_lbl
      break;

    case 0x37:  /* FD_SBPF_OP_DIV64_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | mov rdi, imm
      | div rdi
      | xchg rax, dst64
      break;

    case 0x3c:  /* FD_SBPF_OP_DIV_REG */
      | test src32, src32
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 1
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | div src32
      | xchg eax, dst32
      break;

    case 0x3d:  /* FD_SBPF_OP_JGE_REG */
      | cmp dst64, src64
      | jae =>jmp_dst_lbl
      break;

    case 0x3f:  /* FD_SBPF_OP_DIV64_REG */
      | test src64, src64
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 1
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | div src64
      | xchg rax, dst64
      break;

    /* 0x40 - 0x4f ******************************************************/

    case 0x44:  /* FD_SBPF_OP_OR_IMM */
      | or dst32, imm
      break;

    case 0x45:  /* FD_SBPF_OP_JSET_IMM */
      | test dst64, imm
      | jnz =>jmp_dst_lbl
      break;

    case 0x47:  /* FD_SBPF_OP_OR64_IMM */
      | or dst64, imm
      break;

    case 0x4c:  /* FD_SBPF_OP_OR_REG */
      | or dst32, src32
      break;

    case 0x4d:  /* FD_SBPF_OP_JSET_REG */
      | test dst64, src64
      | jnz =>jmp_dst_lbl
      break;

    case 0x4f:  /* FD_SBPF_OP_OR64_REG */
      | or dst64, src64
      break;

    /* 0x50 - 0x5f ******************************************************/

    case 0x54:  /* FD_SBPF_OP_AND_IMM */
      | and dst32, imm
      break;

    case 0x55:  /* FD_SBPF_OP_JNE_IMM */
      | cmp dst64, imm
      | jne =>jmp_dst_lbl
      break;

    case 0x57:  /* FD_SBPF_OP_AND64_IMM */
      | and dst64, imm
      break;

    case 0x5c:  /* FD_SBPF_OP_AND_REG */
      | and dst32, src32
      break;

    case 0x5d:  /* FD_SBPF_OP_JNE_REG */
      | cmp dst64, src64
      | jne =>jmp_dst_lbl
      break;

    case 0x5f:  /* FD_SBPF_OP_AND64_REG */
      | and dst64, src64
      break;

    /* 0x60 - 0x6f ******************************************************/

    case 0x61:  /* FD_SBPF_OP_LDXW */
      | lea translate_in, [src64+offset]
      | translate_ro_4
      | mov dst32, [translate_out]
      break;

    case 0x62:  /* FD_SBPF_OP_STW */
      | lea translate_in, [dst64+offset]
      | translate_rw_4
      | mov dword [translate_out], imm
      break;

    case 0x63:  /* FD_SBPF_OP_STXW */
      | lea translate_in, [dst64+offset]
      | translate_rw_4
      | mov [translate_out], src32
      break;

    case 0x64:  /* FD_SBPF_OP_LSH_IMM */
      | shl dst32, imm
      break;

    case 0x65:  /* FD_SBPF_OP_JSGT_IMM */
      | cmp dst64, imm
      | jg =>jmp_dst_lbl
      break;

    case 0x67:  /* FD_SBPF_OP_LSH64_IMM */
      | shl dst64, imm
      break;

    case 0x69:  /* FD_SBPF_OP_LDXH */
      | lea translate_in, [src64+offset]
      | translate_ro_2
      | xor dst32, dst32
      | mov Rw(x86_dst), [translate_out]
      break;

    case 0x6a:  /* FD_SBPF_OP_STH */
      | lea translate_in, [dst64+offset]
      | translate_rw_2
      | mov word [translate_out], imm
      break;

    case 0x6b:  /* FD_SBPF_OP_STXH */
      | lea translate_in, [dst64+offset]
      | translate_rw_2
      | mov [translate_out], src32
      break;

    case 0x6c:  /* FD_SBPF_OP_LSH_REG */
      | mov cl, src8
      | shl dst32, cl
      break;

    case 0x6d:  /* FD_SBPF_OP_JSGT_REG */
      | cmp dst64, src64
      | jg =>jmp_dst_lbl
      break;

    case 0x6f:  /* FD_SBPF_OP_LSH64_REG */
      | mov cl, src8
      | shl dst64, cl
      break;

    /* 0x70 - 0x7f ******************************************************/

    case 0x71:  /* FD_SBPF_OP_LDXB */
      | lea translate_in, [src64+offset]
      | translate_ro_1
      /* TODO is there a better way to zero upper and mov byte? */
      | xor dst32, dst32
      | mov Rb(x86_dst), [translate_out]
      break;

    case 0x72:  /* FD_SBPF_OP_STB */
      | lea translate_in, [src64+offset]
      | translate_rw_1
      | mov byte [translate_out], imm
      break;

    case 0x73:  /* FD_SBPF_OP_STXB */
      | lea translate_in, [dst64+offset]
      | translate_rw_1
      | mov byte [translate_out], Rb(x86_src)
      break;

    case 0x74:  /* FD_SBPF_OP_RSH_IMM */
      | shr dst32, imm
      break;

    case 0x75:  /* FD_SBPF_OP_JSGE_IMM */
      | cmp dst64, imm
      | jge =>jmp_dst_lbl
      break;

    case 0x77:  /* FD_SBPF_OP_RSH64_IMM */
      | shr dst64, imm
      break;

    case 0x79:  /* FD_SBPF_OP_LDXQ */
      | lea translate_in, [src64+offset]
      | translate_ro_8
      | mov dst64, [translate_out]
      break;

    case 0x7a:  /* FD_SBPF_OP_STQ */
      | lea translate_in, [dst64+offset]
      | translate_rw_8
      | mov rax, imm
      | mov [translate_out], rax
      break;

    case 0x7b:  /* FD_SBPF_OP_STXQ */
      | lea translate_in, [dst64+offset]
      | translate_rw_8
      | mov [translate_out], src64
      break;

    case 0x7c:  /* FD_SBPF_OP_RSH_REG */
      | mov cl, src8
      | shr dst32, cl
      break;

    case 0x7d:  /* FD_SBPF_OP_JSGE_REG */
      | cmp dst64, src64
      | jge =>jmp_dst_lbl
      break;

    case 0x7f:  /* FD_SBPF_OP_RSH64_REG */
      | mov cl, src8
      | shr dst64, cl
      break;

    /* 0x80-0x8f ********************************************************/

    case 0x84:  /* FD_SBPF_OP_NEG */
      | neg dst32
      break;

    case 0x85: { /* FD_SBPF_OP_CALL_IMM */
      fd_sbpf_syscalls_t const * syscall = fd_sbpf_syscalls_query_const( vm->syscalls, imm, NULL );
      if( !syscall ) {
        ulong target_pc = (ulong)fd_pchash_inverse( imm );
        | call ->call_stack_push
        | call =>target_pc
      } else {
        /* Optimize for code footprint: Generate an offset into the
           syscall table (32-bit) instead of the syscall address (64-bit) */
        | mov rdi, (uint)( (ulong)syscall - (ulong)vm->syscalls );
        | call ->emulate_syscall
      }
      break;
    }

    case 0x87:  /* FD_SBPF_OP_NEG64 */
      | neg dst64
      break;

    case 0x8d:  /* FD_SBPF_OP_CALL_REG */
      FD_LOG_WARNING(( "TODO: CALLX" ));
      break;

    /* 0x90 - 0x9f ******************************************************/

    case 0x94:  /* FD_SBPF_OP_MOD_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | mov edi, imm
      | div edi
      | xchg edx, dst32
      break;

    case 0x95:  /* FD_SBPF_OP_EXIT */
      | call ->call_stack_pop
      | ret
      break;

    case 0x97:  /* FD_SBPF_OP_MOD64_IMM */
      if( FD_UNLIKELY( imm==0 ) ) {
        | jmp ->vm_fault
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | mov rdi, imm
      | div rdi
      | xchg rax, dst64
      break;

    case 0x9c:  /* FD_SBPF_OP_MOD_REG */
      | test src32, src32
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 0
        break;
      }
      | xchg eax, dst32
      | xor edx, edx
      | div src32
      | xchg edx, dst32
      break;

    case 0x9f:  /* FD_SBPF_OP_MOD64_REG */
      | test src64, src64
      | jz ->vm_fault
      if( x86_dst==x86_src ) {
        | mov dst32, 0
        break;
      }
      | xchg rax, dst64
      | xor edx, edx
      | div src64
      | xchg rdx, dst64
      break;

    /* 0xa0 - 0xaf ******************************************************/

    case 0xa4:  /* FD_SBPF_OP_XOR_IMM */
      | xor dst32, imm
      break;

    case 0xa5:  /* FD_SBPF_OP_JLT_IMM */
      | cmp dst64, imm
      | jb =>jmp_dst_lbl
      break;

    case 0xa7:  /* FD_SBPF_OP_XOR64_IMM */
      // TODO sign extension
      | xor dst64, imm
      break;

    case 0xac:  /* FD_SBPF_OP_XOR_REG */
      | xor dst32, src32
      break;

    case 0xad:  /* FD_SBPF_OP_JLT_REG */
      | cmp dst64, src64
      | jb =>jmp_dst_lbl
      break;

    case 0xaf:  /* FD_SBPF_OP_XOR64_REG */
      | xor dst64, src64
      break;

    /* 0xb0 - 0xbf ******************************************************/

    case 0xb4:  /* FD_SBPF_OP_MOV_IMM */
      | mov dst32, imm
      break;

    case 0xb5:  /* FD_SBPF_OP_JLE_IMM */
      | cmp dst64, imm
      | jbe =>jmp_dst_lbl
      break;

    case 0xb7:  /* FD_SBPF_OP_MOV64_IMM */
      if( imm==0 ) {
        | xor dst32, dst32
      } else {
        | mov dst64, imm
      }
      break;

    case 0xbc:  /* FD_SBPF_OP_MOV_REG */
      | mov dst32, src32
      break;

    case 0xbd:  /* FD_SBPF_OP_JLE_REG */
      | cmp dst64, src64
      | jbe =>jmp_dst_lbl
      break;

    case 0xbf:  /* FD_SBPF_OP_MOV64_REG */
      | mov dst64, src64
      break;

    /* 0xc0 - 0xcf ******************************************************/

    case 0xc4:  /* FD_SBPF_OP_ARSH_IMM */
      | sar dst32, imm
      break;

    case 0xc5:  /* FD_SBPF_OP_JSLT_IMM */
      | cmp dst64, imm
      | jl =>jmp_dst_lbl
      break;

    case 0xc7:  /* FD_SBPF_OP_ARSH64_IMM */
      | sar dst64, imm
      break;

    case 0xcc:  /* FD_SBPF_OP_ARSH_REG */
      | mov cl, src8
      | sar dst32, cl
      break;

    case 0xcd:  /* FD_SBPF_OP_JSLT_REG */
      | cmp dst64, src64
      | jl =>jmp_dst_lbl
      break;

    case 0xcf:  /* FD_SBPF_OP_ARSH64_REG */
      | mov cl, src8
      | sar dst64, cl
      break;

    /* 0xd0 - 0xdf ******************************************************/

    case 0xd4:  /* FD_SBPF_OP_END_LE */
      /* nop */
      break;

    case 0xd5:  /* FD_SBPF_OP_JSLE_IMM */
      | cmp dst64, imm
      | jle =>jmp_dst_lbl
      break;

    case 0xdc:  /* FD_SBPF_OP_END_BE */
      switch( imm ) {
      case 16U:
        | movzx dst32, Rw(x86_dst)
        | ror Rw(x86_dst), 8
        break;
      case 32U:
        | bswap dst32
        break;
      case 64U:
        | bswap dst64
        break;
      default:
        break;
        // TODO sigill
      }
      break;

    case 0xdd:  /* FD_SBPF_OP_JSLE_REG */
      | cmp dst64, src64
      | jle =>jmp_dst_lbl
      break;

    default:
      FD_LOG_WARNING(( "Unsupported opcode %x", opcode ));
      cur = text_end;
      break;

    }

  }

  /* Instruction overrun */

  |->overrun: // FIXME
  | jmp ->vm_fault

  /* Finish generating code */

  ulong sz;
  dasm_link( &d, &sz );
  FD_LOG_NOTICE(( "BPF code size: %lu bytes (%#lx)", prog->text_sz, prog->text_sz ));
  FD_LOG_NOTICE(( "x86 code size: %lu bytes (%#lx)", sz, sz ));

  void * buf = mmap( 0, sz, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0 );
  if( FD_UNLIKELY( buf==MAP_FAILED ) ) FD_LOG_ERR(( "mmap failed" ));
  dasm_encode( &d, buf );
  ulong entry_pc    = prog->entry_pc;
  ulong entry_haddr = (ulong)buf + (ulong)dasm_getpclabel( &d, (uint)entry_pc );
  dasm_free( &d );
  mprotect( buf, sz, PROT_READ | PROT_EXEC );

  /* Execute */

  FD_LOG_NOTICE(( "vm at %p", (void *)fd_jit_vm ));

  fd_jit_entrypoint_t jit_entry = (fd_jit_entrypoint_t)( (ulong)fd_jit_labels[ fd_jit_lbl_entrypoint ] );
  FD_LOG_NOTICE(( "x86 code at %p", (void *)(ulong)jit_entry ));

  long dt = -fd_log_wallclock();
  vm->reg[ 1] = 2UL<<32; /* account table address */
  vm->reg[ 2] = 2; /* account count */
  vm->reg[ 3] = 3UL<<32; /* instruction data address */
  vm->reg[ 4] = 0; /* instruction data size */
  vm->reg[10] = (1UL<<32) + 0x1000;
  vm->frame_cnt = 1; /* last exit writes to frame[0] */
  int rc = jit_entry( entry_haddr );
  if( rc==999 ) {
    FD_LOG_ERR(( "Memory access fault: Attempted to access %#lx at %#lx", fd_jit_segfault_vaddr, fd_jit_segfault_rip ));
  }
  FD_LOG_NOTICE(( "Executed program in %g seconds using JIT", (double)(dt+fd_log_wallclock())/1e9 ));

  FD_LOG_HEXDUMP_NOTICE(( "account2", account2, sizeof(account2) ));
  FD_LOG_NOTICE(( "%*s", txn_ctx->log_collector.log_sz, txn_ctx->log_collector.buf ));

  fd_halt();
  return 0;
}
